{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/New-Languages-for-NLP/yiddish/blob/main/research/notebooks/Spacy_Yiddish_Training_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061ba15a"
      },
      "source": [
        "<img width=50% src=\"https://github.com/New-Languages-for-NLP/course-materials/raw/main/w2/using-inception-data/newnlp_notebook.png\" />\n",
        "\n",
        "For full documentation on this project, see [here](https://new-languages-for-nlp.github.io/course-materials/w2/using-inception-data/New%20Language%20Training.html)\n",
        " "
      ],
      "id": "061ba15a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psP7JdcRLR7n"
      },
      "source": [
        "# 1 Prepare the Notebook Environment"
      ],
      "id": "psP7JdcRLR7n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a0ba9e5a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title The Colab runtime comes with spaCy v2 and needs to be upgraded to v3.\n",
        "#@markdown This project uses the GPU by default, if you need to use just the CPU, just uncheck the box below.\n",
        "GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Install spaCy v3 and libraries for GPUs and transformers\n",
        "!pip install spacy --upgrade\n",
        "if GPU:\n",
        "    !pip install 'spacy[transformers,cuda111]'\n",
        "#!pip install wandb spacy-huggingface-hub"
      ],
      "id": "a0ba9e5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfsEKZv6ErlG"
      },
      "source": [
        "The notebook will pull project files from your GitHub repository.  \n",
        "\n",
        "Note that you need to set the langugage (lang), treebank (same as the repo name), test_size and package name in the project.yml file in your repository.  "
      ],
      "id": "WfsEKZv6ErlG"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7c0bda8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "1c195b96-169d-44cc-827d-0ff7c8c26bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'newlang_project' from New-Languages-for-NLP/yiddish\u001b[0m\n",
            "/content/newlang_project\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/newlang_project\n",
            "\u001b[38;5;4mℹ Fetching 1 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/newlang_project/assets/yiddish\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title Enter your language's repository name. \n",
        "#@markdown If the repo is private, please check the \"private_repo\" box and include an [access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).\n",
        "private_repo = False #@param {type:\"boolean\"}\n",
        "repo_name = \"yiddish\" #@param {type:\"string\"}\n",
        "branch = \"main\"\n",
        "\n",
        "\n",
        "!rm -rf /content/newlang_project\n",
        "!rm -rf $repo_name\n",
        "if private_repo:\n",
        "    git_access_token = \"\" #@param {type:\"string\"}\n",
        "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
        "    !git clone $git_url  -b $branch\n",
        "    !cp -r ./$repo_name/newlang_project .  \n",
        "    !mkdir newlang_project/assets/\n",
        "    !mkdir newlang_project/configs/\n",
        "    #!mkdir newlang_project/corpus/\n",
        "    !mkdir newlang_project/metrics/\n",
        "    !mkdir newlang_project/packages/\n",
        "    !mkdir newlang_project/training/\n",
        "    !mkdir newlang_project/assets/$repo_name\n",
        "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
        "    !rm -rf ./$repo_name\n",
        "else:\n",
        "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch $branch\n",
        "    !python -m spacy project assets /content/newlang_project"
      ],
      "id": "7c0bda8e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4dc13741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4eab58-7252-4505-a90e-cf9c23d45864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== install ==================================\u001b[0m\n",
            "Running command: rm -rf lang\n",
            "Running command: mkdir lang\n",
            "Running command: mkdir lang/yi\n",
            "Running command: cp -r assets/yiddish/2_new_language_object/ lang/yi/yi\n",
            "Running command: mv lang/yi/yi/setup.py lang/yi/\n",
            "Running command: /usr/bin/python3 -m pip install -e lang/yi\n",
            "Obtaining file:///content/newlang_project/lang/yi\n",
            "Installing collected packages: yi\n",
            "  Running setup.py develop for yi\n",
            "Successfully installed yi-0.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install the custom language object from Cadet \n",
        "!python -m spacy project run install /content/newlang_project"
      ],
      "id": "4dc13741"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Pretraining**](https://spacy.io/usage/embeddings-transformers#pretraining) (optional)\n",
        "\n",
        "With pretraining, spaCy will learn initial token embeddings from your raw text files.  This can lead to significant improvement when you don't have a lot of data. \n",
        "*  \n",
        "* It will load the raw text files (.txt) from your `0_original_texts` folder in GitHub"
      ],
      "metadata": {
        "id": "6erLtVVNA2VH"
      },
      "id": "6erLtVVNA2VH"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy project run prep-rawtext /content/newlang_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_9jAy8CPfBL",
        "outputId": "86a44ac2-8f22-407f-c696-0d85cd1b814c"
      },
      "id": "Z_9jAy8CPfBL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[38;5;1m✘ Can't find command or workflow 'prep-rawtext' in project.yml\u001b[0m\n",
            "Available commands: install, convert, split, debug, train, evaluate, package,\n",
            "document. Available workflows: all\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy pretrain /content/newlang_project/configs/config_pretrain.cfg ./pretrain --gpu-id 0"
      ],
      "metadata": {
        "id": "dL-cXmmqPcv_",
        "outputId": "a9c85d60-0cac-45ec-8112-2394bfe97b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dL-cXmmqPcv_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: python -m spacy pretrain \n",
            "           [OPTIONS] CONFIG_PATH OUTPUT_DIR\n",
            "Try 'python -m spacy pretrain --help' for help.\n",
            "\n",
            "Error: Invalid value for 'CONFIG_PATH': File '/content/newlang_project/configs/config_pretrain.cfg' does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU_AqRK6LZrF"
      },
      "source": [
        "# 2 Prepare the Data for Training"
      ],
      "id": "qU_AqRK6LZrF"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjD0SN6iAvc",
        "outputId": "a8028a6e-4b99-46fc-fecd-1fbb6fccdeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n"
          ]
        }
      ],
      "source": [
        "#@title (optional) cell to correct a problem when your tokens have no pos value\n",
        "%%writefile /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n",
        "import re\n",
        "\n",
        "from .conll_ner_to_docs import n_sents_info\n",
        "from ...training import iob_to_biluo, biluo_tags_to_spans\n",
        "from ...tokens import Doc, Token, Span\n",
        "from ...vocab import Vocab\n",
        "from wasabi import Printer\n",
        "\n",
        "\n",
        "def conllu_to_docs(\n",
        "    input_data,\n",
        "    n_sents=10,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "    merge_subtokens=False,\n",
        "    no_print=False,\n",
        "    **_\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert conllu files into JSON format for use with train cli.\n",
        "    append_morphology parameter enables appending morphology to tags, which is\n",
        "    useful for languages such as Spanish, where UD tags are not so rich.\n",
        "\n",
        "    Extract NER tags if available and convert them so that they follow\n",
        "    BILUO and the Wikipedia scheme\n",
        "    \"\"\"\n",
        "    MISC_NER_PATTERN = \"^((?:name|NE)=)?([BILU])-([A-Z_]+)|O$\"\n",
        "    msg = Printer(no_print=no_print)\n",
        "    n_sents_info(msg, n_sents)\n",
        "    sent_docs = read_conllx(\n",
        "        input_data,\n",
        "        append_morphology=append_morphology,\n",
        "        ner_tag_pattern=MISC_NER_PATTERN,\n",
        "        ner_map=ner_map,\n",
        "        merge_subtokens=merge_subtokens,\n",
        "    )\n",
        "    sent_docs_to_merge = []\n",
        "    for sent_doc in sent_docs:\n",
        "        sent_docs_to_merge.append(sent_doc)\n",
        "        if len(sent_docs_to_merge) % n_sents == 0:\n",
        "            yield Doc.from_docs(sent_docs_to_merge)\n",
        "            sent_docs_to_merge = []\n",
        "    if sent_docs_to_merge:\n",
        "        yield Doc.from_docs(sent_docs_to_merge)\n",
        "\n",
        "\n",
        "def has_ner(input_data, ner_tag_pattern):\n",
        "    \"\"\"\n",
        "    Check the MISC column for NER tags.\n",
        "    \"\"\"\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            for line in lines:\n",
        "                parts = line.split(\"\\t\")\n",
        "                id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "                for misc_part in misc.split(\"|\"):\n",
        "                    if re.match(ner_tag_pattern, misc_part):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def read_conllx(\n",
        "    input_data,\n",
        "    append_morphology=False,\n",
        "    merge_subtokens=False,\n",
        "    ner_tag_pattern=\"\",\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Yield docs, one for each sentence\"\"\"\n",
        "    vocab = Vocab()  # need vocab to make a minimal Doc\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            doc = conllu_sentence_to_doc(\n",
        "                vocab,\n",
        "                lines,\n",
        "                ner_tag_pattern,\n",
        "                merge_subtokens=merge_subtokens,\n",
        "                append_morphology=append_morphology,\n",
        "                ner_map=ner_map,\n",
        "            )\n",
        "            yield doc\n",
        "\n",
        "\n",
        "def get_entities(lines, tag_pattern, ner_map=None):\n",
        "    \"\"\"Find entities in the MISC column according to the pattern and map to\n",
        "    final entity type with `ner_map` if mapping present. Entity tag is 'O' if\n",
        "    the pattern is not matched.\n",
        "\n",
        "    lines (str): CONLL-U lines for one sentences\n",
        "    tag_pattern (str): Regex pattern for entity tag\n",
        "    ner_map (dict): Map old NER tag names to new ones, '' maps to O.\n",
        "    RETURNS (list): List of BILUO entity tags\n",
        "    \"\"\"\n",
        "    miscs = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_ or \".\" in id_:\n",
        "            continue\n",
        "        miscs.append(misc)\n",
        "\n",
        "    iob = []\n",
        "    for misc in miscs:\n",
        "        iob_tag = \"O\"\n",
        "        for misc_part in misc.split(\"|\"):\n",
        "            tag_match = re.match(tag_pattern, misc_part)\n",
        "            if tag_match:\n",
        "                prefix = tag_match.group(2)\n",
        "                suffix = tag_match.group(3)\n",
        "                if prefix and suffix:\n",
        "                    iob_tag = prefix + \"-\" + suffix\n",
        "                    if ner_map:\n",
        "                        suffix = ner_map.get(suffix, suffix)\n",
        "                        if suffix == \"\":\n",
        "                            iob_tag = \"O\"\n",
        "                        else:\n",
        "                            iob_tag = prefix + \"-\" + suffix\n",
        "                break\n",
        "        iob.append(iob_tag)\n",
        "    return iob_to_biluo(iob)\n",
        "\n",
        "\n",
        "def conllu_sentence_to_doc(\n",
        "    vocab,\n",
        "    lines,\n",
        "    ner_tag_pattern,\n",
        "    merge_subtokens=False,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Create an Example from the lines for one CoNLL-U sentence, merging\n",
        "    subtokens and appending morphology to tags if required.\n",
        "\n",
        "    lines (str): The non-comment lines for a CoNLL-U sentence\n",
        "    ner_tag_pattern (str): The regex pattern for matching NER in MISC col\n",
        "    RETURNS (Example): An example containing the annotation\n",
        "    \"\"\"\n",
        "    # create a Doc with each subtoken as its own token\n",
        "    # if merging subtokens, each subtoken orth is the merged subtoken form\n",
        "    if not Token.has_extension(\"merged_orth\"):\n",
        "        Token.set_extension(\"merged_orth\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_lemma\"):\n",
        "        Token.set_extension(\"merged_lemma\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_morph\"):\n",
        "        Token.set_extension(\"merged_morph\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_spaceafter\"):\n",
        "        Token.set_extension(\"merged_spaceafter\", default=\"\")\n",
        "    words, spaces, tags, poses, morphs, lemmas = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    subtok_word = \"\"\n",
        "    in_subtok = False\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \".\" in id_:\n",
        "            continue\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "            subtok_word = word\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_spaceafter = \"SpaceAfter=No\" not in misc\n",
        "            continue\n",
        "        if merge_subtokens and in_subtok:\n",
        "            words.append(subtok_word)\n",
        "        else:\n",
        "            words.append(word)\n",
        "        if in_subtok:\n",
        "            if id_ == subtok_end:\n",
        "                spaces.append(subtok_spaceafter)\n",
        "            else:\n",
        "                spaces.append(False)\n",
        "        elif \"SpaceAfter=No\" in misc:\n",
        "            spaces.append(False)\n",
        "        else:\n",
        "            spaces.append(True)\n",
        "        if in_subtok and id_ == subtok_end:\n",
        "            subtok_word = \"\"\n",
        "            in_subtok = False\n",
        "        id_ = int(id_) - 1\n",
        "        head = (int(head) - 1) if head not in (\"0\", \"_\") else id_\n",
        "        tag = pos if tag == \"_\" else tag\n",
        "        morph = morph if morph != \"_\" else \"\"\n",
        "        dep = \"ROOT\" if dep == \"root\" else dep\n",
        "        lemmas.append(lemma)\n",
        "        if pos == \"_\":\n",
        "            pos = \"\"\n",
        "        poses.append(pos)\n",
        "        tags.append(tag)\n",
        "        morphs.append(morph)\n",
        "        heads.append(head)\n",
        "        deps.append(dep)\n",
        "\n",
        "    doc = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        lemmas=lemmas,\n",
        "        morphs=morphs,\n",
        "        heads=heads,\n",
        "    )\n",
        "    for i in range(len(doc)):\n",
        "        doc[i]._.merged_orth = words[i]\n",
        "        doc[i]._.merged_morph = morphs[i]\n",
        "        doc[i]._.merged_lemma = lemmas[i]\n",
        "        doc[i]._.merged_spaceafter = spaces[i]\n",
        "    ents = get_entities(lines, ner_tag_pattern, ner_map)\n",
        "    doc.ents = biluo_tags_to_spans(doc, ents)\n",
        "\n",
        "    if merge_subtokens:\n",
        "        doc = merge_conllu_subtokens(lines, doc)\n",
        "\n",
        "    # create final Doc from custom Doc annotation\n",
        "    words, spaces, tags, morphs, lemmas, poses = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    for i, t in enumerate(doc):\n",
        "        words.append(t._.merged_orth)\n",
        "        lemmas.append(t._.merged_lemma)\n",
        "        spaces.append(t._.merged_spaceafter)\n",
        "        morphs.append(t._.merged_morph)\n",
        "        if append_morphology and t._.merged_morph:\n",
        "            tags.append(t.tag_ + \"__\" + t._.merged_morph)\n",
        "        else:\n",
        "            tags.append(t.tag_)\n",
        "        poses.append(t.pos_)\n",
        "        heads.append(t.head.i)\n",
        "        deps.append(t.dep_)\n",
        "\n",
        "    doc_x = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        morphs=morphs,\n",
        "        lemmas=lemmas,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        heads=heads,\n",
        "    )\n",
        "    doc_x.ents = [Span(doc_x, ent.start, ent.end, label=ent.label) for ent in doc.ents]\n",
        "\n",
        "    return doc_x\n",
        "\n",
        "\n",
        "def merge_conllu_subtokens(lines, doc):\n",
        "    # identify and process all subtoken spans to prepare attrs for merging\n",
        "    subtok_spans = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_:\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_span = doc[int(subtok_start) - 1 : int(subtok_end)]\n",
        "            subtok_spans.append(subtok_span)\n",
        "            # create merged tag, morph, and lemma values\n",
        "            tags = []\n",
        "            morphs = {}\n",
        "            lemmas = []\n",
        "            for token in subtok_span:\n",
        "                tags.append(token.tag_)\n",
        "                lemmas.append(token.lemma_)\n",
        "                if token._.merged_morph:\n",
        "                    for feature in token._.merged_morph.split(\"|\"):\n",
        "                        field, values = feature.split(\"=\", 1)\n",
        "                        if field not in morphs:\n",
        "                            morphs[field] = set()\n",
        "                        for value in values.split(\",\"):\n",
        "                            morphs[field].add(value)\n",
        "            # create merged features for each morph field\n",
        "            for field, values in morphs.items():\n",
        "                morphs[field] = field + \"=\" + \",\".join(sorted(values))\n",
        "            # set the same attrs on all subtok tokens so that whatever head the\n",
        "            # retokenizer chooses, the final attrs are available on that token\n",
        "            for token in subtok_span:\n",
        "                token._.merged_orth = token.orth_\n",
        "                token._.merged_lemma = \" \".join(lemmas)\n",
        "                token.tag_ = \"_\".join(tags)\n",
        "                token._.merged_morph = \"|\".join(sorted(morphs.values()))\n",
        "                token._.merged_spaceafter = (\n",
        "                    True if subtok_span[-1].whitespace_ else False\n",
        "                )\n",
        "\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in subtok_spans:\n",
        "            retokenizer.merge(span)\n",
        "\n",
        "    return doc"
      ],
      "id": "HPjD0SN6iAvc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "563fdc94",
        "outputId": "49db5c09-b418-4956-9241-a7ed53707110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== convert ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/convert.py assets/yiddish/3_inception_export 10 yi\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_312_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '33' (equivalent to relative head index: '33'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (23 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1298.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_326_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_35_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_370_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '64' (equivalent to relative head index: '64'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_376_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_335_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_324_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_310_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '31' (equivalent to relative head index: '31'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_38_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_332_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1296.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_360_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_307_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1290.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_315_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_371_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_354_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_304_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_318_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_339_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_351_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_342_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (5 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1053.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (14 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1058.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_313_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '71' (equivalent to relative head index: '71'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_336_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_125_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '22' (equivalent to relative head index: '22'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_331_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_348_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_365_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_362_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '9' with value '22' (equivalent to relative head index: '22'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1056.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_316_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_341_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_34_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_31_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_14_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '42' (equivalent to relative head index: '42'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1057.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1054.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_306_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1052.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_359_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_338_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_344_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_372_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_356_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_328_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_340_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_375_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_352_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (10 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1292.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_317_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_364_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_323_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_325_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1295.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_33_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conllu/Forverts_30_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_314_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_303_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '15' with value '27' (equivalent to relative head index: '27'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_79_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1060.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_309_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_32_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '52' (equivalent to relative head index: '52'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (22 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1297.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_358_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_301_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1293.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_347_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_363_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_368_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1291.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_39_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_374_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_329_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_152_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_378_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1051.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_377_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_349_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_311_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_321_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_350_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_357_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1059.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conllu/processed.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_319_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_36_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_369_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_345_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_187_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_337_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_322_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_327_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_373_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '33' with value '4' (equivalent to relative head index: '4'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_37_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_330_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_379_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '32' with value '36' (equivalent to relative head index: '36'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_57_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '6' with value '21' (equivalent to relative head index: '21'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_355_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_353_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conllu/Forverts_380_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_343_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_305_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 87, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conll_ner_to_docs.py\", line 109, in conll_ner_to_docs\n",
            "    biluo_tags.extend(iob_to_biluo(cols[-1]))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py\", line 13, in iob_to_biluo\n",
            "    out.extend(_consume_ent(tags))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py\", line 46, in _consume_ent\n",
            "    raise ValueError(Errors.E177.format(tag=tag))\n",
            "ValueError: [E177] Ill-formed IOB input detected: V.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_324_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (10 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1292.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_92_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conll/Forverts_30_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_162_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1057.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_327_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_187_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_328_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1060.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (5 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1053.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_173_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_317_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_307_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_325_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_97_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_35_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_18_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_17_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_57_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_305_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/Forverts_37_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_303_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_322_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_314_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_310_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_131_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_321_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/Forverts_311_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_14_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1295.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/Forverts_306_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_79_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_326_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_315_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_44_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1290.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_34_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (14 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1058.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_38_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_125_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1296.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1056.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (22 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1297.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_31_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_36_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1051.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/Forverts_39_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_329_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1052.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1059.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1293.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_313_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_316_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_110_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1054.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_27_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_55_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_304_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_318_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_32_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (23 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1298.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conll/BirobidzhannerShtern_1291.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/Forverts_330_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_301_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_152_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_309_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_319_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_323_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_312_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_33_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/Forverts_114_allPs.spacy\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/convert.py\", line 80, in <module>\n",
            "    typer.run(convert)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 864, in run\n",
            "    app()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 214, in __call__\n",
            "    return get_command(self)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"scripts/convert.py\", line 60, in convert\n",
            "    conllu_bin = DocBin().from_bytes(conllu_file.read_bytes())\n",
            "  File \"/usr/lib/python3.7/pathlib.py\", line 1214, in read_bytes\n",
            "    with self.open(mode='rb') as f:\n",
            "  File \"/usr/lib/python3.7/pathlib.py\", line 1208, in open\n",
            "    opener=self._opener)\n",
            "  File \"/usr/lib/python3.7/pathlib.py\", line 1063, in _opener\n",
            "    return self._accessor.open(self, flags, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/newlang_project/corpus/conllu/Forverts_18_allPs.spacy'\n"
          ]
        }
      ],
      "source": [
        "# Convert the conllu files from inception to spaCy binary format\n",
        "# Read the conll files with ner data and as ents to spaCy docs \n",
        "!python -m spacy project run convert /content/newlang_project"
      ],
      "id": "563fdc94"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9519c858",
        "outputId": "6d9aa476-c59c-4827-c5c2-82dc3862155c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== split ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/split.py 0.2 11 yi\n",
            "🚂 Created 28 training docs\n",
            "😊 Created 6 validation docs\n",
            "🧪  Created 2 test docs\n"
          ]
        }
      ],
      "source": [
        "# test/train split \n",
        "!python -m spacy project run split /content/newlang_project "
      ],
      "id": "9519c858"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4feefe6f",
        "outputId": "928360b5-7998-48c3-8cac-36aa1ec3e694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== debug ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy debug data configs/config.cfg\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: yi\n",
            "Training pipeline: tok2vec, tagger, parser, ner\n",
            "28 training docs\n",
            "6 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;1m✘ Low number of examples to train a new pipeline (28)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 7447 total word(s) in the data (2103 unique)\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 6 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'ORG' (10)\u001b[0m\n",
            "\u001b[2K\u001b[38;5;3m⚠ Low number of examples for label 'm/watch\\' (3)\u001b[0m\n",
            "\u001b[2K\u001b[38;5;3m⚠ Low number of examples for label 'null' (2)\u001b[0m\n",
            "\u001b[2K\u001b[38;5;3m⚠ Low number of examples for label 'm/results\\' (1)\u001b[0m\n",
            "\u001b[2K\u001b[38;5;3m⚠ 43 entity span(s) crossing sentence boundaries\u001b[0m\n",
            "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 16 label(s) in train data\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Dependency Parsing =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 7447 sentence(s) with an average length of 1.0 words.\u001b[0m\n",
            "\u001b[38;5;4mℹ 1 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;4mℹ 1 label(s) in projectivized train data\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 5 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 5 warnings\u001b[0m\n",
            "\u001b[38;5;1m✘ 1 error\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Debug the data\n",
        "!python -m spacy project run debug /content/newlang_project "
      ],
      "id": "4feefe6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "151-cj1dLgAD"
      },
      "source": [
        "# 3 Model Training "
      ],
      "id": "151-cj1dLgAD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKW2QGPTJ_CZ"
      },
      "source": [
        "If your project file uses Weights and Biases to monitor model training, you'll need to create an account at [wandb.ai](https://wandb.ai/site) and get an API key.  "
      ],
      "id": "KKW2QGPTJ_CZ"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "38b490a4",
        "outputId": "74e1187d-78c0-4d10-c250-05c20dc5e52e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/config.cfg --output training/yiddish --gpu-id 0 --nlp.lang=yi\n",
            "\u001b[38;5;2m✔ Created output directory: training/yiddish\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/yiddish\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-05-12 15:15:00,808] [INFO] Set up nlp object from config\n",
            "[2022-05-12 15:15:00,822] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "[2022-05-12 15:15:00,828] [INFO] Created vocabulary\n",
            "[2022-05-12 15:15:00,829] [INFO] Finished initializing nlp object\n",
            "[2022-05-12 15:16:13,000] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  -----------  -----------  --------  -------  -------  -------  -------  ------  ------  ------  ------\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "  0       0          0.00       210.94         0.00    106.77    53.57   100.00     0.00   100.00    0.00    0.00    0.00    0.35\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "ValueError('Could not find gold transition - see logs above.')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1156, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/transition_parser.pyx\", line 406, in spacy.pipeline.transition_parser.Parser.update\n",
            "  File \"spacy/pipeline/transition_parser.pyx\", line 518, in spacy.pipeline.transition_parser.Parser.get_batch_loss\n",
            "  File \"spacy/pipeline/_parser_internals/arc_eager.pyx\", line 827, in spacy.pipeline._parser_internals.arc_eager.ArcEager.set_costs\n",
            "ValueError: Could not find gold transition - see logs above.\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "!python -m spacy project run train /content/newlang_project "
      ],
      "id": "38b490a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynXr8vlXqCxv"
      },
      "source": [
        "If you get `ValueError: Could not find gold transition - see logs above.`  \n",
        "You may not have sufficent data to train on: https://github.com/explosion/spaCy/discussions/7282"
      ],
      "id": "ynXr8vlXqCxv"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "018362d8",
        "outputId": "f6380cfc-8b83-4bae-b9ab-9fdec63e0ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate ./training/yiddish/model-best ./corpus/converted/test.spacy --output ./metrics/yiddish.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK      100.00\n",
            "TAG      52.88 \n",
            "UAS      100.00\n",
            "LAS      0.00  \n",
            "NER P    0.00  \n",
            "NER R    0.00  \n",
            "NER F    0.00  \n",
            "SENT P   100.00\n",
            "SENT R   100.00\n",
            "SENT F   100.00\n",
            "SPEED    1878  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== LAS (per type) ===============================\u001b[0m\n",
            "\n",
            "          P      R      F\n",
            "_      0.00   0.00   0.00\n",
            "root   0.00   0.00   0.00\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P      R      F\n",
            "PER   0.00   0.00   0.00\n",
            "LOC   0.00   0.00   0.00\n",
            "ORG   0.00   0.00   0.00\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to metrics/yiddish.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using the test data\n",
        "!python -m spacy project run evaluate /content/newlang_project "
      ],
      "id": "018362d8"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gg1sLlVrgiyu",
        "outputId": "29421a49-3caa-4fb7-eda1-873e3447bc38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.cfg  meta.json  ner  parser  tagger  tok2vec  tokenizer\tvocab\n"
          ]
        }
      ],
      "source": [
        "# Find the path for your meta.json file\n",
        "# You'll need to add newlang_project/ +  the path from the training step just after \"✔ Saved pipeline to output directory\"\n",
        "!ls /content/newlang_project/training/yiddish/model-last"
      ],
      "id": "gg1sLlVrgiyu"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3zGCTURr9JE6"
      },
      "outputs": [],
      "source": [
        "#Update meta.json\n",
        "import spacy \n",
        "import srsly \n",
        "\n",
        "# Change path to match that from the training cell where it says \"✔ Saved pipeline to output directory\"\n",
        "meta_path = \"/content/newlang_project/training/yiddish/model-last/meta.json\"\n",
        "\n",
        "# Replace values below for your project\n",
        "my_meta = { \n",
        "    \"lang\":\"yi\",\n",
        "    \"name\":\"yiddish_sm\",\n",
        "    \"version\":\"0.0.1\",\n",
        "    \"description\":\"Yiddish pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, lemmatizer.\",\n",
        "    \"author\":\"New Languages for NLP\",\n",
        "    \"email\":\"newnlp@princeton.edu\",\n",
        "    \"url\":\"https://newnlp.princeton.edu\",\n",
        "    \"license\":\"MIT\", \n",
        "    }\n",
        "meta = spacy.util.load_meta(meta_path)\n",
        "meta.update(my_meta)\n",
        "srsly.write_json(meta_path, meta)"
      ],
      "id": "3zGCTURr9JE6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM309FhNVAeb"
      },
      "source": [
        "### Download the trained model to your computer.\n"
      ],
      "id": "JM309FhNVAeb"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8e1d6f36",
        "outputId": "0e3ea6e3-014f-4138-9c57-6c63abf364d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Building package artifacts: sdist\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "/content/newlang_project/training/yiddish/model-last/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory 'yi_yiddish_sm-0.0.1'\u001b[0m\n",
            "newlang_project/export/yi_yiddish_sm-0.0.1\n",
            "running sdist\n",
            "running egg_info\n",
            "creating yi_yiddish_sm.egg-info\n",
            "writing yi_yiddish_sm.egg-info/PKG-INFO\n",
            "writing dependency_links to yi_yiddish_sm.egg-info/dependency_links.txt\n",
            "writing entry points to yi_yiddish_sm.egg-info/entry_points.txt\n",
            "writing requirements to yi_yiddish_sm.egg-info/requires.txt\n",
            "writing top-level names to yi_yiddish_sm.egg-info/top_level.txt\n",
            "writing manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "reading manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "running check\n",
            "creating yi_yiddish_sm-0.0.1\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying files to yi_yiddish_sm-0.0.1...\n",
            "copying MANIFEST.in -> yi_yiddish_sm-0.0.1\n",
            "copying README.md -> yi_yiddish_sm-0.0.1\n",
            "copying meta.json -> yi_yiddish_sm-0.0.1\n",
            "copying setup.py -> yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/__init__.py -> yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "copying yi_yiddish_sm/meta.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "copying yi_yiddish_sm.egg-info/PKG-INFO -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/SOURCES.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/dependency_links.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/entry_points.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/not-zip-safe -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/requires.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/top_level.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/README.md -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/config.cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/meta.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tokenizer -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/moves -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/moves -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/key2row -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/lookups.bin -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/strings.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/vectors -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/vectors.cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "Writing yi_yiddish_sm-0.0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'yi_yiddish_sm-0.0.1' (and everything under it)\n",
            "\u001b[38;5;2m✔ Successfully created zipped Python package\u001b[0m\n",
            "newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Save the model to disk in a format that can be easily  downloaded and re-used.\n",
        "!python -m spacy package /content/newlang_project/training/yiddish/model-last newlang_project/export "
      ],
      "id": "8e1d6f36"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8d32abf2",
        "outputId": "5fe8e9a1-e0b8-4c50-9024-0a9d12e3e497"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b248d12d-96bc-4a07-9afa-17c6316c0f4c\", \"yi_yiddish_sm-0.0.1.tar.gz\", 7712876)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# replace with the path in the previous cell under \"✔ Successfully created zipped Python package\"\n",
        "files.download('newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz')\n",
        "\n",
        "# once on your computer, you can pip install yi_yiddish_sm-0.0.1.tar.gz\n",
        "# Be sure to add the file to the 4_trained_models folder in GitHub"
      ],
      "id": "8d32abf2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using your trained model "
      ],
      "metadata": {
        "id": "xVpfmRBBVA-R"
      },
      "id": "xVpfmRBBVA-R"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the model as a module \n",
        "!pip install newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n",
        "# If you're using Jupyter or Colab, you may need to restart the runtime after installation of your model for it to be available. \n"
      ],
      "metadata": {
        "id": "vWJ-wz_5VCsZ",
        "outputId": "f4d6386c-789e-43ee-f6e5-3e51f749ce66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vWJ-wz_5VCsZ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n",
            "Requirement already satisfied: spacy<3.4.0,>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from yi-yiddish-sm==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (0.9.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (8.0.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (0.6.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0->yi-yiddish-sm==0.0.1) (2.0.1)\n",
            "\u001b[33mDEPRECATION: Source distribution is being reinstalled despite an installed package having the same name and version as the installed package. pip 21.2 will remove support for this functionality. A possible replacement is use --force-reinstall. You can find discussion regarding this at https://github.com/pypa/pip/issues/8711.\u001b[0m\n",
            "Building wheels for collected packages: yi-yiddish-sm\n",
            "  Building wheel for yi-yiddish-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yi-yiddish-sm: filename=yi_yiddish_sm-0.0.1-py3-none-any.whl size=7715422 sha256=f108b2bdd9ad93982be3819ddbdb6137b800a1d2053a5f67fa5f9c59c1a9e855\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/bc/62/9998d02ad7ee2fbf9ddb7eaeea603e1b3d81ddcb68e635cb90\n",
            "Successfully built yi-yiddish-sm\n",
            "Installing collected packages: yi-yiddish-sm\n",
            "  Attempting uninstall: yi-yiddish-sm\n",
            "    Found existing installation: yi-yiddish-sm 0.0.1\n",
            "    Uninstalling yi-yiddish-sm-0.0.1:\n",
            "      Successfully uninstalled yi-yiddish-sm-0.0.1\n",
            "Successfully installed yi-yiddish-sm-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To import the model, use the language code (in this case 'yi' followed by the name (\"yiddish_sm\")) \n",
        "# You don't need the version information: '-0.0.1'\n",
        "#So ✔ Successfully created package 'yi_yiddish_sm-0.0.1'\n",
        "# Becomes yi_yiddish_sm \n",
        "import spacy \n",
        "nlp = spacy.load('yi_yiddish_sm')\n",
        "doc = nlp(\"איך האב א היים אין ישראל\")\n",
        "for token in doc:\n",
        "  print(token.text,token.pos_)\n",
        "for ent in doc.ents:\n",
        "  print(ent)"
      ],
      "metadata": {
        "id": "0otKujKlcNzn",
        "outputId": "8730ac9c-2d7d-4f2e-e91d-e3f49b3be32f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0otKujKlcNzn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "איך \n",
            "האב \n",
            "א \n",
            "היים \n",
            "אין \n",
            "ישראל \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"איך האב א היים אין תל-אביב\")\n",
        "for token in doc:\n",
        "  print(token.text,token.pos_)\n",
        "for ent in doc.ents:\n",
        "  print(ent)"
      ],
      "metadata": {
        "id": "ijjwfY9p4Sbt",
        "outputId": "fda9c0e8-70f7-4ff8-ac68-a837ef0980a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ijjwfY9p4Sbt",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "איך \n",
            "האב \n",
            "א \n",
            "היים \n",
            "אין \n",
            "תל \n",
            "- \n",
            "אביב \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"איך וווין נישט אין תל־אביב\")\n",
        "for token in doc:\n",
        "  print(token.text,token.pos_)\n",
        "for ent in doc.ents:\n",
        "  print(ent)"
      ],
      "metadata": {
        "id": "5iwLnXLG4XS6",
        "outputId": "7501d100-a1cc-4cb9-ecb8-64d64ffa17c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5iwLnXLG4XS6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "איך \n",
            "וווין \n",
            "נישט \n",
            "אין \n",
            "תל־אביב \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2dQ-14R24rXv"
      },
      "id": "2dQ-14R24rXv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "New Language Training (Colab).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}